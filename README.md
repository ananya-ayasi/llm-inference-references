# LLM Inference

A curated list of resources and articles on efficient and scalable LLM inference and deployment:

- **[BentoML for LLM Serving](https://bentoml.com/llm/)**  
  Covers everything to know about LLM inference, from core concepts and performance metrics, to optimization techniques and operation best practices.

- **[Efficient Model Serving with Fractional H100 GPUs](https://www.baseten.co/blog/using-fractional-h100-gpus-for-efficient-model-serving/#how-multi-instance-gpus-work)**  
  Baseten’s blog post explains how fractional GPU allocation and multi-instance GPU usage can optimize inference workloads.

- **[Deploying LLMs into Production using TensorRT-LLM](https://towardsdatascience.com/deploying-llms-into-production-using-tensorrt-llm-ed36e620dac4/)**  
  A step-by-step guide to productionizing LLMs with TensorRT-based optimizations.

- **[Frameworks for Serving LLMs](https://medium.com/better-programming/frameworks-for-serving-llms-60b7f7b23407)**  
  Medium article overviewing various frameworks for deploying and serving language models.

- **[Peeling Back—Paper/Journal Disaggregated Serving](https://michalpitr.substack.com/p/paper-journal-disaggragated-serving)**  
  Michal Pitr’s take on modeling inference pipelines with disaggregated serving architectures.

- **[Inference Scale Diffusion](https://inference-scale-diffusion.github.io/)**  
  A web resource detailing diffusion-based inference scaling techniques.

- **[Streaming LLM via HanLab MIT](https://hanlab.mit.edu/blog/streamingllm)**  
  MIT’s Han Lab blog post on streaming strategies for LLM inference.
